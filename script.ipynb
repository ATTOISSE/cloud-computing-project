{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd \n",
    "from google.cloud import bigquery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"isi-group-m2-dsia\"\n",
    "BUCKET_NAME = \"m2dsia-attoisse-mohamed-data\"\n",
    "TABLE_ID = f\"{PROJECT_ID}.dataset_attoisse_mohamed.transactions\"\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.get_bucket(BUCKET_NAME)\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/\n",
      "input/error1.csv\n",
      "input/error2.csv\n",
      "input/error3.csv\n",
      "input/transaction.csv\n",
      "input/transactions.csv\n"
     ]
    }
   ],
   "source": [
    "blobs = bucket.list_blobs(prefix=\"input/\")\n",
    "for blob in blobs:\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_path, destination_blob_name):\n",
    "\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "\n",
    "    blob.upload_from_filename(file_path)\n",
    "    \n",
    "    print(f\"Fichier '{file_path}' upload√© vers {destination_blob_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier 'files/errors/error3.csv' upload√© vers input/error3.csv.\n"
     ]
    }
   ],
   "source": [
    "upload_file('files/errors/error3.csv','input/error3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file(filename):\n",
    "    try:\n",
    "        blob = bucket.blob(filename)\n",
    "        if blob.exists():\n",
    "            blob.delete()  \n",
    "            print(f\"Le fichier {filename} a √©t√© supprim√© avec succ√®s.\")\n",
    "        else:\n",
    "            print(f\"Le fichier {filename} n'existe pas dans le bucket.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la suppression du fichier {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "delete_file('input/transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• T√©l√©chargement d'un fichier depuis Cloud Storage\n",
    "def download_blob(source_blob_name, destination_file_name):\n",
    "    \"\"\"T√©l√©charge un fichier depuis Cloud Storage vers un fichier local temporaire.\"\"\"\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    print(f\"üì• {source_blob_name} t√©l√©charg√© en local.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì§ Upload d'un fichier ou d'un contenu texte dans Cloud Storage\n",
    "def upload_blob(content, destination_blob_name):\n",
    "    \"\"\"Upload un fichier ou une donn√©e string dans Cloud Storage.\"\"\"\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    if isinstance(content, str):  \n",
    "        blob.upload_from_string(content, content_type='text/csv')\n",
    "    else:  \n",
    "        blob.upload_from_string(content)\n",
    "    \n",
    "    print(f\"üì§ {destination_blob_name} upload√© dans {BUCKET_NAME}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Nettoie les donn√©es du DataFrame en √©vitant les avertissements Pandas.\"\"\"\n",
    "    required_columns = ['transaction_id', 'product_name', 'category', 'price', 'quantity', 'date']\n",
    "    \n",
    "    if df[required_columns].isnull().any().any():\n",
    "        raise ValueError(\"‚ùå Champs obligatoires manquants. Impossible de nettoyer les donn√©es.\")\n",
    "\n",
    "    df = df.copy()  \n",
    "    df['customer_name'] = df['customer_name'].fillna(df['customer_name'].mode()[0])\n",
    "    df['customer_email'] = df['customer_email'].fillna(df['customer_email'].mode()[0])\n",
    "\n",
    "    df['transaction_id'] = df['transaction_id'].astype(int)\n",
    "    df['product_name'] = df['product_name'].astype(str)\n",
    "    df['category'] = df['category'].astype(str)\n",
    "    df['price'] = df['price'].astype(float)\n",
    "    df['quantity'] = df['quantity'].astype(int)\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ°Ô∏è Validation des donn√©es\n",
    "def validate_data(df):\n",
    "    \"\"\"V√©rifie que les donn√©es sont valides.\"\"\"\n",
    "    required_columns = ['transaction_id', 'product_name', 'category', 'price', 'quantity', 'date']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        return False\n",
    "    \n",
    "    if df[required_columns].isnull().any().any():\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        df['transaction_id'] = df['transaction_id'].astype(int)\n",
    "        df['product_name'] = df['product_name'].astype(str)\n",
    "        df['category'] = df['category'].astype(str)\n",
    "        df['price'] = df['price'].astype(float)\n",
    "        df['quantity'] = df['quantity'].astype(int)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Chargement des donn√©es dans BigQuery\n",
    "def load_to_bigquery(df):\n",
    "    \"\"\"Charge un DataFrame Pandas dans une table BigQuery.\"\"\"\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND  # Ajoute les nouvelles donn√©es\n",
    "    )\n",
    "    \n",
    "    job = bigquery_client.load_table_from_dataframe(df, TABLE_ID, job_config=job_config)\n",
    "    job.result()  \n",
    "    \n",
    "    print(f\"‚úÖ Donn√©es charg√©es dans BigQuery : {TABLE_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# üîÑ Traitement d'un fichier Cloud Storage\n",
    "def process_cloud_file(cloud_file_path):\n",
    "    \"\"\"T√©l√©charge, traite et charge un fichier dans BigQuery.\"\"\"\n",
    "    local_tmp_dir = tempfile.gettempdir()  \n",
    "    local_file_path = os.path.join(local_tmp_dir, cloud_file_path.split('/')[-1])\n",
    "    download_blob(cloud_file_path, local_file_path)  # T√©l√©chargement\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(local_file_path)\n",
    "\n",
    "        if not validate_data(df):\n",
    "            raise ValueError(\"Donn√©es invalides. Champs obligatoires manquants ou types incorrects.\")\n",
    "\n",
    "        df_cleaned = clean_data(df)\n",
    "\n",
    "        clean_file_path = f\"clean/{cloud_file_path.split('/')[-1]}\"\n",
    "        upload_blob(df_cleaned.to_csv(index=False), clean_file_path)\n",
    "\n",
    "        load_to_bigquery(df_cleaned)\n",
    "\n",
    "        delete_file(clean_file_path)\n",
    "\n",
    "        done_file_path = f\"done/{cloud_file_path.split('/')[-1]}\"\n",
    "        upload_blob(open(local_file_path, 'rb').read(), done_file_path)\n",
    "\n",
    "        print(f\"‚úÖ {cloud_file_path} trait√© et d√©plac√© vers clean/ et done/.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_file_path = f\"error/{cloud_file_path.split('/')[-1]}\"\n",
    "        upload_blob(open(local_file_path, 'rb').read(), error_file_path)\n",
    "\n",
    "        print(f\"‚ùå Erreur lors du traitement de {cloud_file_path}. D√©plac√© vers error/. Erreur : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Lister les fichiers dans `input/`\n",
    "def list_files_in_input():\n",
    "    \"\"\"Liste tous les fichiers dans le dossier 'input/' du bucket.\"\"\"\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blobs = bucket.list_blobs(prefix=\"input/\")  # Liste les fichiers dans input/\n",
    "    \n",
    "    files = [blob.name for blob in blobs if not blob.name.endswith(\"/\")]\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÅ MAIN\n",
    "def main():\n",
    "    files = list_files_in_input()\n",
    "\n",
    "    if not files:\n",
    "        print(\"üîé Aucun fichier √† traiter dans input/.\")\n",
    "        return\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        i += 1\n",
    "        print('processus du fichier',i)\n",
    "        process_cloud_file(file)\n",
    "        print()\n",
    "    print('Fin du traitement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processus du fichier 1\n",
      "üì• input/error1.csv t√©l√©charg√© en local.\n",
      "üì§ error/error1.csv upload√© dans m2dsia-attoisse-mohamed-data.\n",
      "‚ùå Erreur lors du traitement de input/error1.csv. D√©plac√© vers error/. Erreur : Donn√©es invalides. Champs obligatoires manquants ou types incorrects.\n",
      "\n",
      "processus du fichier 2\n",
      "üì• input/error2.csv t√©l√©charg√© en local.\n",
      "üì§ error/error2.csv upload√© dans m2dsia-attoisse-mohamed-data.\n",
      "‚ùå Erreur lors du traitement de input/error2.csv. D√©plac√© vers error/. Erreur : Donn√©es invalides. Champs obligatoires manquants ou types incorrects.\n",
      "\n",
      "processus du fichier 3\n",
      "üì• input/error3.csv t√©l√©charg√© en local.\n",
      "üì§ error/error3.csv upload√© dans m2dsia-attoisse-mohamed-data.\n",
      "‚ùå Erreur lors du traitement de input/error3.csv. D√©plac√© vers error/. Erreur : Donn√©es invalides. Champs obligatoires manquants ou types incorrects.\n",
      "\n",
      "processus du fichier 4\n",
      "üì• input/transaction.csv t√©l√©charg√© en local.\n",
      "üì§ clean/transaction.csv upload√© dans m2dsia-attoisse-mohamed-data.\n",
      "‚úÖ Donn√©es charg√©es dans BigQuery : isi-group-m2-dsia.dataset_attoisse_mohamed.transactions\n",
      "Le fichier clean/transaction.csv a √©t√© supprim√© avec succ√®s.\n",
      "üì§ done/transaction.csv upload√© dans m2dsia-attoisse-mohamed-data.\n",
      "‚úÖ input/transaction.csv trait√© et d√©plac√© vers clean/ et done/.\n",
      "\n",
      "Fin du traitement\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
