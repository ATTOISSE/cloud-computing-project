{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd \n",
    "from google.cloud import bigquery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"isi-group-m2-dsia\"\n",
    "BUCKET_NAME = \"m2dsia-attoisse-mohamed-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.get_bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/\n",
      "input/transactions.csv\n"
     ]
    }
   ],
   "source": [
    "blobs = bucket.list_blobs(prefix=\"input/\")\n",
    "for blob in blobs:\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(data, destination_blob_name):\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    if isinstance(data, str):\n",
    "        blob.upload_from_string(data, content_type='text/csv')\n",
    "    else:\n",
    "        blob.upload_from_string(data, content_type='application/octet-stream')\n",
    "    \n",
    "    print(f\"Fichier uploadé vers {destination_blob_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file(filename):\n",
    "    try:\n",
    "        blob = bucket.blob(filename)\n",
    "        if blob.exists():\n",
    "            blob.delete()  \n",
    "            print(f\"Le fichier {filename} a été supprimé avec succès.\")\n",
    "        else:\n",
    "            print(f\"Le fichier {filename} n'existe pas dans le bucket.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la suppression du fichier {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier input/transactions.csv a été supprimé avec succès.\n"
     ]
    }
   ],
   "source": [
    "delete_file('input/transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    required_columns = ['transaction_id', 'product_name', 'category', 'price', 'quantity', 'date']\n",
    "    if df[required_columns].isnull().any().any():\n",
    "        raise ValueError(\"Champs obligatoires manquants. Impossible de nettoyer les données.\")\n",
    "    \n",
    "    mode_customer_name = df['customer_name'].mode()[0]  \n",
    "    df['customer_name'] = df['customer_name'].fillna(mode_customer_name)\n",
    "    \n",
    "    mode_customer_email = df['customer_email'].mode()[0]  \n",
    "    df['customer_email'] = df['customer_email'].fillna(mode_customer_email)\n",
    "\n",
    "    df['transaction_id'] = df['transaction_id'].astype(int)  \n",
    "    df['product_name'] = df['product_name'].astype(str)   \n",
    "    df['category'] = df['category'].astype(str)   \n",
    "    df['price'] = df['price'].astype(float)  \n",
    "    df['quantity'] = df['quantity'].astype(int)  \n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date   \n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df):\n",
    "    required_columns = ['transaction_id', 'product_name', 'category', 'price', 'quantity', 'date']\n",
    "    if not all(column in df.columns for column in required_columns):\n",
    "        return False\n",
    "\n",
    "    if df[required_columns].isnull().any().any():\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        df['transaction_id'] = df['transaction_id'].astype(int)  \n",
    "        df['product_name'] = df['product_name'].astype(str)   \n",
    "        df['category'] = df['category'].astype(str)   \n",
    "        df['price'] = df['price'].astype(float)  \n",
    "        df['quantity'] = df['quantity'].astype(int)  \n",
    "        df['date'] = pd.to_datetime(df['date']).dt.date  \n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if not validate_data(df):\n",
    "            raise ValueError(\"Données invalides. Champs obligatoires manquants ou types incorrects.\")\n",
    "        \n",
    "        df_cleaned = clean_data(df)\n",
    "        \n",
    "        destination_path = f\"clean/{file_path.split('/')[-1]}\"\n",
    "        upload_blob(df_cleaned.to_csv(index=False), destination_path)\n",
    "        print(f\"Fichier {file_path} traité et déplacé vers clean/.\")\n",
    "        \n",
    "        return df_cleaned\n",
    "    except Exception as e:\n",
    "        destination_path = f\"error/{file_path.split('/')[-1]}\"\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_content = file.read()\n",
    "            upload_blob(file_content, destination_path)\n",
    "        \n",
    "        print(f\"Erreur lors du traitement du fichier {file_path}. Déplacé vers error/. Erreur : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_bigquery(df, table_id):\n",
    "    \"\"\"\n",
    "    Ajoute les données du DataFrame à une table BigQuery.\n",
    "    \n",
    "    :param df: DataFrame contenant les données à ajouter.\n",
    "    :param table_id: ID de la table BigQuery (ex: \"project_id.dataset_id.table_id\").\n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  \n",
    "    \n",
    "    print(f\"Données ajoutées à la table BigQuery : {table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_path, table_id):\n",
    "    \n",
    "    try:\n",
    "        df_process = process_file(file_path)\n",
    "        add_to_bigquery(df_process, table_id)\n",
    "        destination_path = f\"done/{file_path.split('/')[-1]}\" \n",
    "        upload_blob(df_process.to_csv(index=False), destination_path)\n",
    "        print(f\"Fichier {file_path} traité, ajouté à la table BigQuery, et déplacé vers done/.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        destination_path = f\"error/{file_path.split('/')[-1]}\"  \n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_content = file.read()\n",
    "            upload_blob(file_content, destination_path)\n",
    "        print(f\"Erreur lors du traitement du fichier {file_path}. Déplacé vers error/. Erreur : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier uploadé vers error/error1.csv.\n",
      "Erreur lors du traitement du fichier files/errors/error1.csv. Déplacé vers error/. Erreur : Données invalides. Champs obligatoires manquants ou types incorrects.\n",
      "Fichier uploadé vers error/error1.csv.\n",
      "Erreur lors du traitement du fichier files/errors/error1.csv. Déplacé vers error/. Erreur : 'NoneType' object has no attribute 'columns'\n"
     ]
    }
   ],
   "source": [
    "file_to_process = \"files/errors/error1.csv\"  \n",
    "table_id = f\"{PROJECT_ID}.dataset_attoisse_mohamed.transactions\"\n",
    "main(file_to_process,table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
